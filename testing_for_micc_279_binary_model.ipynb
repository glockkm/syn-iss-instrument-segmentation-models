{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1152,
     "status": "ok",
     "timestamp": 1700597624342,
     "user": {
      "displayName": "Kimberly Glock",
      "userId": "00419033501391390114"
     },
     "user_tz": 480
    },
    "id": "EJhh8AbxPEpc",
    "outputId": "fdf77ef0-7c79-4d68-b495-3a70193c4b12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# === GOOGLE DRIVE MOUNT ===\n",
    "# Mount Google Drive to access datasets, model checkpoints, and results\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# store data here for notebook access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gz-XXp7tPZXM"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "from torch.utils.data import Dataset\n",
    "# dataset class in pytorch which is the data that will get fed into the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HMA2ghePZh7"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBuBtVmQeZNg"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61XQKDECPZuS"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "from torch.utils.data import Dataset as BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uj-m_3iVXDGs"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import albumentations as albu\n",
    "# https://albumentations.ai/docs/api_reference/augmentations/geometric/resize/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlLhyHjvXDJg"
   },
   "outputs": [],
   "source": [
    "# === DATA AUGMENTATION AND PREPROCESSING ===\n",
    "# Define augmentation and preprocessing pipelines for validation and inference\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(544, 960)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "# Assuming the following functions and constants are defined correctly elsewhere in the code\n",
    "# ENCODER, ENCODER_WEIGHTS, ACTIVATION, CLASSES, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noHYJ134TBEh"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import re\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BaseObject(nn.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__()\n",
    "        self._name = name\n",
    "\n",
    "    @property\n",
    "    def __name__(self):\n",
    "        if self._name is None:\n",
    "            name = self.__class__.__name__\n",
    "            s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n",
    "            return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()\n",
    "        else:\n",
    "            return self._name\n",
    "\n",
    "\n",
    "class Metric(BaseObject):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Loss(BaseObject):\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Loss):\n",
    "            return SumOfLosses(self, other)\n",
    "        else:\n",
    "            raise ValueError(\"Loss should be inherited from `Loss` class\")\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __mul__(self, value):\n",
    "        if isinstance(value, (int, float)):\n",
    "            return MultipliedLoss(self, value)\n",
    "        else:\n",
    "            raise ValueError(\"Loss should be inherited from `BaseLoss` class\")\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "\n",
    "class SumOfLosses(Loss):\n",
    "    def __init__(self, l1, l2):\n",
    "        name = \"{} + {}\".format(l1.__name__, l2.__name__)\n",
    "        super().__init__(name=name)\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def __call__(self, *inputs):\n",
    "        return self.l1.forward(*inputs) + self.l2.forward(*inputs)\n",
    "\n",
    "\n",
    "class MultipliedLoss(Loss):\n",
    "    def __init__(self, loss, multiplier):\n",
    "\n",
    "        # resolve name\n",
    "        if len(loss.__name__.split(\"+\")) > 1:\n",
    "            name = \"{} * ({})\".format(multiplier, loss.__name__)\n",
    "        else:\n",
    "            name = \"{} * {}\".format(multiplier, loss.__name__)\n",
    "        super().__init__(name=name)\n",
    "        self.loss = loss\n",
    "        self.multiplier = multiplier\n",
    "\n",
    "    def __call__(self, *inputs):\n",
    "        return self.multiplier * self.loss.forward(*inputs)\n",
    "class Activation(nn.Module):\n",
    "\n",
    "    def __init__(self, name, **params):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if name is None or name == 'identity':\n",
    "            self.activation = nn.Identity(**params)\n",
    "        elif name == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif name == 'softmax2d':\n",
    "            self.activation = nn.Softmax(dim=1, **params)\n",
    "        elif name == 'softmax':\n",
    "            self.activation = nn.Softmax(**params)\n",
    "        elif name == 'logsoftmax':\n",
    "            self.activation = nn.LogSoftmax(**params)\n",
    "        elif name == 'argmax':\n",
    "            self.activation = ArgMax(**params)\n",
    "        elif name == 'argmax2d':\n",
    "            self.activation = ArgMax(dim=1, **params)\n",
    "        elif callable(name):\n",
    "            self.activation = name(**params)\n",
    "        else:\n",
    "            raise ValueError('Activation should be callable/sigmoid/softmax/logsoftmax/None; got {}'.format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIJ5TaaIOMKW"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import torch\n",
    "\n",
    "\n",
    "def _take_channels(*xs, ignore_channels=None):\n",
    "    if ignore_channels is None:\n",
    "        return xs\n",
    "    else:\n",
    "        channels = [channel for channel in range(xs[0].shape[1]) if channel not in ignore_channels]\n",
    "        xs = [torch.index_select(x, dim=1, index=torch.tensor(channels).to(x.device)) for x in xs]\n",
    "        return xs\n",
    "\n",
    "\n",
    "def _threshold(x, threshold=None):\n",
    "    if threshold is not None:\n",
    "        return (x > threshold).type(x.dtype)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def iou(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n",
    "    \"\"\"Calculate Intersection over Union between ground truth and prediction\n",
    "    Args:\n",
    "        pr (torch.Tensor): predicted tensor\n",
    "        gt (torch.Tensor):  ground truth tensor\n",
    "        eps (float): epsilon to avoid zero division\n",
    "        threshold: threshold for outputs binarization\n",
    "    Returns:\n",
    "        float: IoU (Jaccard) score\n",
    "    \"\"\"\n",
    "\n",
    "    pr = _threshold(pr, threshold=threshold)\n",
    "    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n",
    "\n",
    "    intersection = torch.sum(gt * pr)\n",
    "    union = torch.sum(gt) + torch.sum(pr) - intersection + eps\n",
    "    return (intersection + eps) / union\n",
    "\n",
    "\n",
    "jaccard = iou\n",
    "\n",
    "\n",
    "def f_score(pr, gt, beta=1, eps=1e-7, threshold=None, ignore_channels=None):\n",
    "    \"\"\"Calculate F-score between ground truth and prediction\n",
    "    Args:\n",
    "        pr (torch.Tensor): predicted tensor\n",
    "        gt (torch.Tensor):  ground truth tensor\n",
    "        beta (float): positive constant\n",
    "        eps (float): epsilon to avoid zero division\n",
    "        threshold: threshold for outputs binarization\n",
    "    Returns:\n",
    "        float: F score\n",
    "    \"\"\"\n",
    "\n",
    "    pr = _threshold(pr, threshold=threshold)\n",
    "    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n",
    "\n",
    "    tp = torch.sum(gt * pr)\n",
    "    fp = torch.sum(pr) - tp\n",
    "    fn = torch.sum(gt) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + eps) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def accuracy(pr, gt, threshold=0.5, ignore_channels=None):\n",
    "    \"\"\"Calculate accuracy score between ground truth and prediction\n",
    "    Args:\n",
    "        pr (torch.Tensor): predicted tensor\n",
    "        gt (torch.Tensor):  ground truth tensor\n",
    "        eps (float): epsilon to avoid zero division\n",
    "        threshold: threshold for outputs binarization\n",
    "    Returns:\n",
    "        float: precision score\n",
    "    \"\"\"\n",
    "    pr = _threshold(pr, threshold=threshold)\n",
    "    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n",
    "\n",
    "    tp = torch.sum(gt == pr, dtype=pr.dtype)\n",
    "    score = tp / gt.view(-1).shape[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "def precision(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n",
    "    \"\"\"Calculate precision score between ground truth and prediction\n",
    "    Args:\n",
    "        pr (torch.Tensor): predicted tensor\n",
    "        gt (torch.Tensor):  ground truth tensor\n",
    "        eps (float): epsilon to avoid zero division\n",
    "        threshold: threshold for outputs binarization\n",
    "    Returns:\n",
    "        float: precision score\n",
    "    \"\"\"\n",
    "\n",
    "    pr = _threshold(pr, threshold=threshold)\n",
    "    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n",
    "\n",
    "    tp = torch.sum(gt * pr)\n",
    "    fp = torch.sum(pr) - tp\n",
    "\n",
    "    score = (tp + eps) / (tp + fp + eps)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def recall(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n",
    "    \"\"\"Calculate Recall between ground truth and prediction\n",
    "    Args:\n",
    "        pr (torch.Tensor): A list of predicted elements\n",
    "        gt (torch.Tensor):  A list of elements that are to be predicted\n",
    "        eps (float): epsilon to avoid zero division\n",
    "        threshold: threshold for outputs binarization\n",
    "    Returns:\n",
    "        float: recall score\n",
    "    \"\"\"\n",
    "\n",
    "    pr = _threshold(pr, threshold=threshold)\n",
    "    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n",
    "\n",
    "    tp = torch.sum(gt * pr)\n",
    "    fn = torch.sum(gt) - tp\n",
    "\n",
    "    score = (tp + eps) / (tp + fn + eps)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDtU_ccZOMP8"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import torch.nn as nn\n",
    "\n",
    "class JaccardLoss(Loss):\n",
    "    def __init__(self, eps=1.0, activation=None, ignore_channels=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "        self.activation = Activation(activation)\n",
    "        self.ignore_channels = ignore_channels\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        y_pr = self.activation(y_pr)\n",
    "        return 1 - jaccard(\n",
    "            y_pr,\n",
    "            y_gt,\n",
    "            eps=self.eps,\n",
    "            threshold=None,\n",
    "            ignore_channels=self.ignore_channels,\n",
    "        )\n",
    "\n",
    "\n",
    "class DiceLoss(Loss):\n",
    "    def __init__(self, eps=1.0, beta=1.0, activation=None, ignore_channels=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "        self.beta = beta\n",
    "        self.activation = Activation(activation)\n",
    "        self.ignore_channels = ignore_channels\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        y_pr = self.activation(y_pr)\n",
    "        return 1 - f_score(\n",
    "            y_pr,\n",
    "            y_gt,\n",
    "            beta=self.beta,\n",
    "            eps=self.eps,\n",
    "            threshold=None,\n",
    "            ignore_channels=self.ignore_channels,\n",
    "        )\n",
    "\n",
    "\n",
    "class L1Loss(nn.L1Loss, Loss):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MSELoss(nn.MSELoss, Loss):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(nn.CrossEntropyLoss, Loss):\n",
    "    pass\n",
    "\n",
    "\n",
    "class NLLLoss(nn.NLLLoss, Loss):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BCELoss(nn.BCELoss, Loss):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BCEWithLogitsLoss(nn.BCEWithLogitsLoss, Loss):\n",
    "    pass\n",
    "\n",
    "    # https://www.scitepress.org/Papers/2021/103040/103040.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoOcVDNOOMSl"
   },
   "outputs": [],
   "source": [
    "# === EVALUATION ===\n",
    "# Compute IoU, Dice coefficient, or other metrics for quantitative evaluation\n",
    "class IoU(Metric):\n",
    "    __name__ = \"iou_score\"\n",
    "\n",
    "    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "        self.threshold = threshold\n",
    "        self.activation = Activation(activation)\n",
    "        self.ignore_channels = ignore_channels\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        y_pr = self.activation(y_pr)\n",
    "        return iou(\n",
    "            y_pr,\n",
    "            y_gt,\n",
    "            eps=self.eps,\n",
    "            threshold=self.threshold,\n",
    "            ignore_channels=self.ignore_channels,\n",
    "        )\n",
    "\n",
    "\n",
    "class Fscore(Metric):\n",
    "    def __init__(self, beta=1, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "        self.activation = Activation(activation)\n",
    "        self.ignore_channels = ignore_channels\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        y_pr = self.activation(y_pr)\n",
    "        return f_score(\n",
    "            y_pr,\n",
    "            y_gt,\n",
    "            eps=self.eps,\n",
    "            beta=self.beta,\n",
    "            threshold=self.threshold,\n",
    "            ignore_channels=self.ignore_channels,\n",
    "        )\n",
    "\n",
    "\n",
    "class Accuracy(Metric):\n",
    "    def __init__(self, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.activation = Activation(activation)\n",
    "        self.ignore_channels = ignore_channels\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        y_pr = self.activation(y_pr)\n",
    "        return accuracy(\n",
    "            y_pr,\n",
    "            y_gt,\n",
    "            threshold=self.threshold,\n",
    "            ignore_channels=self.ignore_channels,\n",
    "        )\n",
    "\n",
    "\n",
    "class Recall(Metric):\n",
    "    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "        self.threshold = threshold\n",
    "        self.activation = Activation(activation)\n",
    "        self.ignore_channels = ignore_channels\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        y_pr = self.activation(y_pr)\n",
    "        return recall(\n",
    "            y_pr,\n",
    "            y_gt,\n",
    "            eps=self.eps,\n",
    "            threshold=self.threshold,\n",
    "            ignore_channels=self.ignore_channels,\n",
    "        )\n",
    "\n",
    "\n",
    "class Precision(Metric):\n",
    "    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "        self.threshold = threshold\n",
    "        self.activation = Activation(activation)\n",
    "        self.ignore_channels = ignore_channels\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        y_pr = self.activation(y_pr)\n",
    "        return precision(\n",
    "            y_pr,\n",
    "            y_gt,\n",
    "            eps=self.eps,\n",
    "            threshold=self.threshold,\n",
    "            ignore_channels=self.ignore_channels,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ARtxmRXOMVY"
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    IoU(threshold=0.5),\n",
    "    Accuracy(threshold=0.5),\n",
    "    Fscore(threshold=0.5),\n",
    "    Recall(threshold=0.5),\n",
    "    Precision(threshold=0.5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13654,
     "status": "ok",
     "timestamp": 1700597694984,
     "user": {
      "displayName": "Kimberly Glock",
      "userId": "00419033501391390114"
     },
     "user_tz": 480
    },
    "id": "f6zySyJUPZZ5",
    "outputId": "e923bad8-fdb2-4f41-99ff-f4de69de18ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
      "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-1kz44wvx\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-1kz44wvx\n",
      "  Resolved https://github.com/qubvel/segmentation_models.pytorch to commit 6db76a1106426ac5b55f39fba68168f3bccae7f8\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch==0.3.3) (0.16.0+cu118)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch==0.3.3) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch==0.3.3) (0.7.1)\n",
      "Requirement already satisfied: timm==0.9.7 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch==0.3.3) (0.9.7)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch==0.3.3) (4.66.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch==0.3.3) (9.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch==0.3.3) (1.16.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (2.1.0+cu118)\n",
      "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.3.3) (4.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch==0.3.3) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch==0.3.3) (0.19.4)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch==0.3.3) (0.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.3.3) (1.23.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.3.3) (2.31.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.7->segmentation-models-pytorch==0.3.3) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.3.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.3.3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.3.3) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.3.3) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# === MODEL LOADING ===\n",
    "# Load the pre-trained binary segmentation model and set to evaluation mode\n",
    "# install and Clone Github segmentation models pytorch\n",
    "!pip install git+https://github.com/qubvel/segmentation_models.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8SOWA_hTBM4"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8FMVTS1TBPY"
   },
   "outputs": [],
   "source": [
    "# === DATA AUGMENTATION AND PREPROCESSING ===\n",
    "# Define augmentation and preprocessing pipelines for validation and inference\n",
    "# resnext50_32x4d, mit_b2, timm-gernet_s, efficientnet-b3, mobilenet_v2, resnet152, vgg13\t\t# EXPERIMENT HERE WITH TRUE DATA\n",
    "ENCODER = 'efficientnet-b3' # extracts feature map and increases channels\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'sigmoid'\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "# decoders= PAN, PSPNet, MAnet, Linknet, FPN, DeepLabV3, DeepLabV3Plus, Unet  # upsamples feature maps and decreases channels and outputs segmentation maps    # EXPERIMENT HERE WITH TRUE DATA\n",
    "model =smp.FPN(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    classes=len(CLASSES),\n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "# normalize the data the same way as during encoder weight pre-training\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "\n",
    "# smp is the library for encoders and decoders\n",
    "# resnet encoder commonly used in image segmentation\n",
    "# pspnet architecture: can use any encoder, used for semantic segmentation, cnn/pyramid pooling\n",
    "# pan architecture (pyramid attention network): good for semantic segmentation\n",
    "# unet arhcitecture: semantic segmentation for biomedical images\n",
    "# do: resnet50/unet or ++, efficientnet-b3/fpn, mobilenet_v2/pspnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w_I3JfzTBVA"
   },
   "outputs": [],
   "source": [
    "# === MODEL LOADING ===\n",
    "# Load the pre-trained binary segmentation model and set to evaluation mode\n",
    "# define optimization algorithm with learning rate # EXPERIMENT WITH RMSPROP AND ADAM\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.parameters(), lr=0.001), #0.0001 # EXPERIMENT HERE WITH TRUE DATA\n",
    "])\n",
    "\n",
    "# define loss function\n",
    "loss = BCEWithLogitsLoss() #DiceLoss() # EXPERIMENT HERE WITH BOTH LOSSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlCGEOxFPo-d"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPhpytkcPpGe"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Meter(object):\n",
    "    \"\"\"Meters provide a way to keep track of important statistics in an online manner.\n",
    "    This class is abstract, but provides a standard interface for all meters to follow.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the meter to default settings.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def add(self, value):\n",
    "        \"\"\"Log a new value to the meter\n",
    "        Args:\n",
    "            value: Next result to include.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def value(self):\n",
    "        \"\"\"Get the value of the meter in the current state.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class AverageValueMeter(Meter):\n",
    "    def __init__(self):\n",
    "        super(AverageValueMeter, self).__init__()\n",
    "        self.reset()\n",
    "        self.val = 0\n",
    "\n",
    "    def add(self, value, n=1):\n",
    "        self.val = value\n",
    "        self.sum += value\n",
    "        self.var += value * value\n",
    "        self.n += n\n",
    "\n",
    "        if self.n == 0:\n",
    "            self.mean, self.std = np.nan, np.nan\n",
    "        elif self.n == 1:\n",
    "            self.mean = 0.0 + self.sum  # this is to force a copy in torch/numpy\n",
    "            self.std = np.inf\n",
    "            self.mean_old = self.mean\n",
    "            self.m_s = 0.0\n",
    "        else:\n",
    "            self.mean = self.mean_old + (value - n * self.mean_old) / float(self.n)\n",
    "            self.m_s += (value - self.mean_old) * (value - self.mean)\n",
    "            self.mean_old = self.mean\n",
    "            self.std = np.sqrt(self.m_s / (self.n - 1.0))\n",
    "\n",
    "    def value(self):\n",
    "        return self.mean, self.std\n",
    "\n",
    "    def reset(self):\n",
    "        self.n = 0\n",
    "        self.sum = 0.0\n",
    "        self.var = 0.0\n",
    "        self.val = 0.0\n",
    "        self.mean = np.nan\n",
    "        self.mean_old = 0.0\n",
    "        self.m_s = 0.0\n",
    "        self.std = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoLDc_HNPpJY"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm as tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJ0wxFopgpUG"
   },
   "outputs": [],
   "source": [
    "# === INFERENCE ===\n",
    "# Run inference on test images and generate segmentation predictions\n",
    "class Epoch:\n",
    "    def __init__(self, model, loss, metrics, stage_name, device=\"cpu\", verbose=True):\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.stage_name = stage_name\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "\n",
    "        self._to_device()\n",
    "\n",
    "    def _to_device(self):\n",
    "        self.model.to(self.device)\n",
    "        if self.loss is not None:\n",
    "            self.loss.to(self.device)\n",
    "        for metric in self.metrics:\n",
    "            metric.to(self.device)\n",
    "\n",
    "    def _format_logs(self, logs):\n",
    "        str_logs = [\"{} - {:.4}\".format(k, v) for k, v in logs.items()]\n",
    "        s = \", \".join(str_logs)\n",
    "        return s\n",
    "\n",
    "    def batch_update(self, x, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, dataloader, is_test=False):\n",
    "        self.on_epoch_start()\n",
    "\n",
    "        if is_test:\n",
    "            return self._run_test(dataloader)\n",
    "\n",
    "        logs = {}\n",
    "        loss_meter = AverageValueMeter()\n",
    "        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}\n",
    "\n",
    "        with tqdm(\n",
    "            dataloader,\n",
    "            desc=self.stage_name,\n",
    "            file=sys.stdout,\n",
    "            disable=not (self.verbose),\n",
    "        ) as iterator:\n",
    "            for x, y in iterator:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                loss, y_pred = self.batch_update(x, y)\n",
    "\n",
    "                # update loss logs\n",
    "                loss_value = loss.cpu().detach().numpy()\n",
    "                loss_meter.add(loss_value)\n",
    "                loss_logs = {self.loss.__name__: loss_meter.mean}\n",
    "                logs.update(loss_logs)\n",
    "\n",
    "                # update metrics logs\n",
    "                for metric_fn in self.metrics:\n",
    "                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()\n",
    "                    metrics_meters[metric_fn.__name__].add(metric_value)\n",
    "                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\n",
    "                logs.update(metrics_logs)\n",
    "\n",
    "                if self.verbose:\n",
    "                    s = self._format_logs(logs)\n",
    "                    iterator.set_postfix_str(s)\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def _run_test(self, dataloader):\n",
    "        predictions = []\n",
    "        with tqdm(\n",
    "            dataloader,\n",
    "            desc=\"test\",\n",
    "            file=sys.stdout,\n",
    "            disable=not self.verbose\n",
    "        ) as iterator:\n",
    "            for batch in iterator:\n",
    "                x = batch.to(self.device)\n",
    "                # print(\"Batch shape in _run_test:\", x.shape)  # Debugging line\n",
    "                y_pred = self.batch_update(x)\n",
    "                print(\"Number of predictions in this batch:\", y_pred.size(0))  # assuming y_pred is of shape [batch_size, ...]\n",
    "                predictions.extend(y_pred.cpu())  # use extend if y_pred contains multiple images\n",
    "                # predictions.append(y_pred.cpu())\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eLSkEt_g1-l"
   },
   "outputs": [],
   "source": [
    "# === MODEL LOADING ===\n",
    "# Load the pre-trained binary segmentation model and set to evaluation mode\n",
    "class ValidEpoch(Epoch):\n",
    "    def __init__(self, model, loss, metrics, device=\"cpu\", verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name=\"valid\",\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    # def batch_update(self, x, y=None):\n",
    "    #     with torch.no_grad():\n",
    "    #         prediction = self.model.forward(x)\n",
    "    #         return prediction\n",
    "\n",
    "    def batch_update(self, x, y=None):\n",
    "        with torch.no_grad():\n",
    "            print(\"Input shape in batch_update:\", x.shape)  # Debugging line\n",
    "            prediction = self.model.forward(x)\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Hx9Gq-HPpXB"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7oelaCpXUJx"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "# validation transformations are used for test data because they are less aggressive and maintain the original content of the image, which is important for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSFXJB5dpVMZ"
   },
   "outputs": [],
   "source": [
    "# === DATASET AND DATALOADER ===\n",
    "# Define or load dataset class and data loader for testing and evaluation\n",
    "\n",
    "\n",
    "class TestDataset(BaseDataset):\n",
    "    \"\"\"Dataset for testing. Read images, apply preprocessing transformations.\"\"\"\n",
    "\n",
    "    def __init__(self, images_dir, augmentation=None, preprocessing=None):\n",
    "        self.ids = sorted(os.listdir(images_dir))\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image)\n",
    "            image = sample['image']\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image)\n",
    "            image = sample['image']\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOWbCCPbbjyi"
   },
   "outputs": [],
   "source": [
    "# def get_test_augmentation():\n",
    "#     \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "#     test_transform = [\n",
    "#         albu.PadIfNeeded(544, 960)\n",
    "#     ]\n",
    "#     return albu.Compose(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6fFoFuWVYSG"
   },
   "outputs": [],
   "source": [
    "# === DATASET AND DATALOADER ===\n",
    "# Define or load dataset class and data loader for testing and evaluation\n",
    "\n",
    "\n",
    "test_dataset = TestDataset(\n",
    "    images_dir='/content/drive/My Drive/Colab Notebooks/micc-278/real_images_test/',\n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n",
    "\n",
    "batch_size = 4  # adjust this based on system's capabilities\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1700603474757,
     "user": {
      "displayName": "Kimberly Glock",
      "userId": "00419033501391390114"
     },
     "user_tz": 480
    },
    "id": "LeOgOFM5wBbO",
    "outputId": "98e4fb8a-db53-4b5d-eec8-707ef3f7df3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([4, 3, 544, 960])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    x = batch\n",
    "    print(\"Batch shape:\", x.shape)  # Should print [batch_size, C, H, W]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN44q9Vm347y"
   },
   "outputs": [],
   "source": [
    "print(\"Total number of images in dataset:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BJMO87-z__X"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import os\n",
    "\n",
    "save_dir = '/content/drive/My Drive/Colab Notebooks/micc-278/PredictedImages'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3961,
     "status": "ok",
     "timestamp": 1700603481823,
     "user": {
      "displayName": "Kimberly Glock",
      "userId": "00419033501391390114"
     },
     "user_tz": 480
    },
    "id": "e9HgNKtuwfA_",
    "outputId": "46c81848-2c97-4e64-bc51-13cfe0f91e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "test:   0%|          | 0/12 [00:00<?, ?it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:   8%|▊         | 1/12 [00:00<00:03,  2.85it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:  17%|█▋        | 2/12 [00:00<00:03,  3.00it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:  25%|██▌       | 3/12 [00:00<00:02,  3.08it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:  33%|███▎      | 4/12 [00:01<00:02,  3.15it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:  42%|████▏     | 5/12 [00:01<00:02,  3.21it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:  50%|█████     | 6/12 [00:01<00:01,  3.24it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:  58%|█████▊    | 7/12 [00:02<00:01,  3.29it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:  67%|██████▋   | 8/12 [00:02<00:01,  3.33it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:  75%|███████▌  | 9/12 [00:02<00:00,  3.31it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:  83%|████████▎ | 10/12 [00:03<00:00,  3.28it/s]Input shape in batch_update: torch.Size([4, 3, 544, 960])\n",
      "Number of predictions in this batch: 4\n",
      "test:  92%|█████████▏| 11/12 [00:03<00:00,  3.28it/s]Input shape in batch_update: torch.Size([3, 3, 544, 960])\n",
      "Number of predictions in this batch: 3\n",
      "test: 100%|██████████| 12/12 [00:03<00:00,  3.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# === MODEL LOADING ===\n",
    "# Load the pre-trained binary segmentation model and set to evaluation mode\n",
    "Trained_model = torch.load('/content/drive/My Drive/Colab Notebooks/challenge_data_trained_binary_model.pth')\n",
    "test_epoch = ValidEpoch(\n",
    "    model=Trained_model,\n",
    "    loss=None,  # Omit loss if predicting without calculating loss\n",
    "    metrics=metrics,  # Assuming metrics is defined, otherwise set it to []\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "predictions = test_epoch.run(test_dataloader, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwQAd6JjETID"
   },
   "outputs": [],
   "source": [
    "# test_dataset = cv.copyMakeBorder(test_dataset,0,0,2,2,cv.BORDER_REPLICATE)\n",
    "# https://docs.opencv.org/4.x/d3/df2/tutorial_py_basic_ops.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_uNl8Ky4rf0"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    # invert colors: change instruments to white and background to black\n",
    "    inverted_prediction = 1 - prediction\n",
    "\n",
    "    # ensure the tensor values are still in the range [0, 1]\n",
    "    inverted_prediction = torch.clamp(inverted_prediction, 0, 1)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f'prediction_{i}.png')\n",
    "    save_image(inverted_prediction, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYcvBuPuY2Je"
   },
   "outputs": [],
   "source": [
    "# === INFERENCE ===\n",
    "# Run inference on test images and generate segmentation predictions\n",
    "for i, prediction in enumerate(predictions):\n",
    "    # invert colors: change instruments to white and background to black\n",
    "    inverted_prediction = 1 - prediction\n",
    "\n",
    "    # ensure the tensor values are still in the range [0, 1]\n",
    "    inverted_prediction = torch.clamp(inverted_prediction, 0, 1)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f'prediction_{i}.png')\n",
    "    save_image(inverted_prediction, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHikTE1s3jhm"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "# import os\n",
    "# from torchvision.utils import save_image\n",
    "\n",
    "# for i, prediction in enumerate(predictions):\n",
    "#     save_path = os.path.join(save_dir, f'prediction_{i}.png')\n",
    "#     save_image(prediction, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IQGOBowsjGF"
   },
   "outputs": [],
   "source": [
    "# === INFERENCE ===\n",
    "# Run inference on test images and generate segmentation predictions\n",
    "# IoU = intersection over union (overlap)\n",
    "# y_pred = model.predict(an_x_test_item)\n",
    "# y_pred_thresholded = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8fRueM6sz92"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "# from tensorflow.keras.metrics import MeanIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jjl3Ssxls1iQ"
   },
   "outputs": [],
   "source": [
    "# === INFERENCE ===\n",
    "# Run inference on test images and generate segmentation predictions\n",
    "# n_classes = 2\n",
    "# IOU_keras = MeanIoU(num_classes=n_classes)\n",
    "# IOU_keras.update_state(y_pred_thresholded, y_test)\n",
    "# print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "# 1.0 avg is a red flag....maybe the 0 and 1 for target are switched and it's predicting for black????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7jPwXE8s61w"
   },
   "outputs": [],
   "source": [
    "# === INFERENCE ===\n",
    "# Run inference on test images and generate segmentation predictions\n",
    "# threshold = 0.5\n",
    "# test_img_number = random.randint(0, len(X_test)-1)\n",
    "# test_img = X_test[test_img_number]\n",
    "# ground_truth=y_test[test_img_number]\n",
    "# test_img_input=np.expand_dims(test_img, 0)\n",
    "# print(test_img_input.shape)\n",
    "# prediction = (model.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
    "# print(prediction.shape)\n",
    "\n",
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(231)\n",
    "# plt.title('Testing Image')\n",
    "# plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "# plt.subplot(232)\n",
    "# plt.title('Testing Label')\n",
    "# plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
    "# plt.subplot(233)\n",
    "# plt.title('Prediction on test image')\n",
    "# plt.imshow(prediction, cmap='gray')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4ni0nnJqvhc"
   },
   "outputs": [],
   "source": [
    "# # plot the training and validation accuracy and loss at each epoch\n",
    "# loss = Trained_model.history['loss']\n",
    "# val_loss = Trained_model.history['val_loss']\n",
    "# epochs = range(1, len(loss) + 1)\n",
    "# plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# acc = Trained_model.history['accuracy']\n",
    "# val_acc = Trained_model.history['val_accuracy']\n",
    "# plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNLLZwUJVX2d"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "# Import necessary libraries for model loading, dataset handling, and visualization\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXOOJZNsY7Mn"
   },
   "outputs": [],
   "source": [
    "results_dict = {'Metric Name': [], 'Value': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v4XXOYrY7Tv"
   },
   "outputs": [],
   "source": [
    "for metric_name, metric_value in logs.items():\n",
    "    results_dict['Metric Name'].append(metric_name)\n",
    "    results_dict['Value'].append(metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_FEDO4jY7Wl",
    "outputId": "ed9d2cb5-4da2-4537-ea28-bb0d087aea89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Metric Name     Value\n",
      "0  bce_with_logits_loss  0.374519\n",
      "1             iou_score  0.997403\n",
      "2              accuracy  0.997863\n",
      "3                fscore  0.998700\n",
      "4                recall  0.998260\n",
      "5             precision  0.999140\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_dict)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aHOUMsSbvxF"
   },
   "outputs": [],
   "source": [
    "results_df.to_csv('/content/drive/My Drive/Colab Notebooks/MICC-284-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_A0Zpkekc3e1"
   },
   "outputs": [],
   "source": [
    "# === MODEL LOADING ===\n",
    "# Load the pre-trained binary segmentation model and set to evaluation mode\n",
    "for i in range(len(test_dataset)):\n",
    "  image, gt_mask = test_dataset[i]\n",
    "\n",
    "  x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "  predicted_mask = Trained_model.predict(x_tensor)\n",
    "\n",
    "  pr_mask = (predicted_mask.squeeze().cpu().numpy().round())\n",
    "  pr_mask = pr_mask[:,:]    #[1,:,:]\n",
    "  gt_mask = gt_mask[0,:]    #[1,:,:]\n",
    "  image_t = image.transpose(1, 2, 0)\n",
    "  visualizeData(\n",
    "    image=image_t,\n",
    "    ground_truth_mask=gt_mask,\n",
    "    predicted_mask=pr_mask\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZsgjMugfbAz"
   },
   "outputs": [],
   "source": [
    "# === MODEL LOADING ===\n",
    "# Load the pre-trained binary segmentation model and set to evaluation mode\n",
    "# # get orignial image and mask from test dataset\n",
    "# image, gt_mask = test_dataset[9]  # change the index number to try out diff test images\n",
    "\n",
    "# x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "# predicted_mask = Trained_model.predict(x_tensor)\n",
    "\n",
    "# pr_mask = (predicted_mask.squeeze().cpu().numpy().round())\n",
    "# pr_mask = pr_mask[:,:]    #[1,:,:]\n",
    "# gt_mask = gt_mask[0,:]    #[1,:,:]\n",
    "# image_t = image.transpose(1, 2, 0)\n",
    "# visualizeData(\n",
    "#         image=image_t,\n",
    "#         ground_truth_mask=gt_mask,\n",
    "#         predicted_mask=pr_mask\n",
    "#     )\n",
    "# This error occurs when I try to access an element in a NumPy array using too many indices. The error message specifically means that I am trying to index a 2-dimensional NumPy array with 3 indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXaUdx4_bj1Z",
    "outputId": "41c203d0-366b-4167-df2a-74bedc97c934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instrument']\n"
     ]
    }
   ],
   "source": [
    "# === INFERENCE ===\n",
    "# Run inference on test images and generate segmentation predictions\n",
    "# convert the predicted mask to numpy and get the predicted class indices\n",
    "# predicted_output = torch.argmax(predicted_mask.squeeze(), dim=0).detach().cpu().numpy()\n",
    "# Indices = np.unique(predicted_output)\n",
    "\n",
    "# # print(Indices)\n",
    "# print(CLASSES)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
